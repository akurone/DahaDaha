{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hazırlık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29996,\n",
       " 30,\n",
       " 30,\n",
       " 'abaca',\n",
       " (tensor([0, 1, 2, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0]),\n",
       "  tensor([ 1,  2,  1,  3,  1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gerekli kütüphaneler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# bunlar bize hep lazım\n",
    "alfabe = list('.abcçdefgğhıijklmnoöprsştuüvyz')\n",
    "harf2idx = { harf:idx for idx, harf in enumerate(alfabe) }\n",
    "idx2harf = { idx:harf for harf, idx in harf2idx.items() }\n",
    "\n",
    "isimler = open(\"./isimler.txt\", \"r\").read().splitlines()\n",
    "maxUzunluk = max([len(isim) for isim in isimler])\n",
    "\n",
    "def isle(isim):\n",
    "  enc, ln = torch.tensor([harf2idx[h] for h in isim], dtype=torch.long), len(isim)\n",
    "  x, y = torch.zeros(maxUzunluk + 1, dtype=torch.long), torch.zeros(maxUzunluk + 1, dtype=torch.long)\n",
    "  x[1:1+ln] = enc\n",
    "  y[:ln] = enc\n",
    "  y[ln+1:] = -1\n",
    "  return x, y\n",
    "\n",
    "def xyOlustur(isimler):\n",
    "  X, Y = [], []\n",
    "  for isim in isimler:\n",
    "    x, y = isle(isim)\n",
    "    X.append(x) ; Y.append(y)\n",
    "  X, Y = torch.stack(X), torch.stack(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n_alfabe = len(alfabe)\n",
    "n_isim = len(isimler)\n",
    "n_isim, n_alfabe, maxUzunluk, isimler[0], isle(isimler[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri Kümeleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29996, 31]) torch.Size([29996, 31])\n",
      "torch.Size([23996, 31]) torch.Size([23996, 31])\n"
     ]
    }
   ],
   "source": [
    "tumX, tumY = xyOlustur(isimler)\n",
    "random.seed(5) ; karisik = random.sample(isimler, k = int(0.8 * n_isim))\n",
    "trnX, trnY = xyOlustur(karisik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soyutlamalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module): # https://arxiv.org/abs/1606.08415\n",
    "  def forward(self, x):\n",
    "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module): # as in GPT-2\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    assert config.n_embd % config.n_head == 0\n",
    "    # key, query, value projections for all heads, but in a batch\n",
    "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "    # output projection\n",
    "    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "    # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                   .view(1, 1, config.block_size, config.block_size))\n",
    "    self.n_head = config.n_head\n",
    "    self.n_embd = config.n_embd\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "    q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "    att = F.softmax(att, dim=-1)\n",
    "    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "    # output projection\n",
    "    y = self.c_proj(y)\n",
    "    return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "    self.attn = CausalSelfAttention(config)\n",
    "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "    self.mlp = nn.ModuleDict(dict(\n",
    "      c_fc  = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "      c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "      act   = NewGELU(),\n",
    "    ))\n",
    "    m = self.mlp\n",
    "    self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.attn(self.ln_1(x))\n",
    "    x = x + self.mlpf(self.ln_2(x))\n",
    "    return x\n",
    "\n",
    "class Transformer(nn.Module): # as seen in GPT-2\n",
    "\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.block_size = config.block_size\n",
    "\n",
    "    self.transformer = nn.ModuleDict(dict(\n",
    "      wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "      wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "      h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "      ln_f = nn.LayerNorm(config.n_embd),\n",
    "    ))\n",
    "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    b, t = idx.size()\n",
    "    assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "    pos = torch.arange(0, t, dtype=torch.long).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "    # forward the GPT model itself\n",
    "    tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "    pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "    x = tok_emb + pos_emb\n",
    "    for block in self.transformer.h:\n",
    "      x = block(x)\n",
    "    x = self.transformer.ln_f(x)\n",
    "    logits = self.lm_head(x)\n",
    "    loss = None if targets is None else F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  @torch.inference_mode()\n",
    "  @torch.no_grad()\n",
    "  def generate(self, n_sample):\n",
    "    self.eval()\n",
    "    n_blk, gen = self.block_size, torch.zeros(n_sample, 1, dtype=torch.long)\n",
    "    def intrim(x):\n",
    "      y = x[1:].tolist() ; y = y[:y.index(0) if 0 in y else len(y)] ; return y\n",
    "    for _ in range(n_blk - 1):\n",
    "      logits, _ = self(gen if gen.size(1) <= n_blk else gen[:, -n_blk:])\n",
    "      nxt = torch.multinomial(F.softmax(logits[:, -1, :], dim = -1), num_samples = 1)\n",
    "      gen = torch.cat((gen, nxt), dim = 1)\n",
    "    self.train()\n",
    "    return map(intrim, gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ve Parametreler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205888\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "  block_size: int = None\n",
    "  vocab_size: int = None\n",
    "  batch_size: int = 32\n",
    "  n_layer: int = 4\n",
    "  n_head: int = 4\n",
    "  n_embd: int = n_head * 16\n",
    "  weight_decay: float = 0.01\n",
    "  learning_rate: float = 5e-4\n",
    "\n",
    "config = ModelConfig(vocab_size = n_alfabe, block_size = maxUzunluk + 1)\n",
    "model = Transformer(config)\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.607516050338745\n",
      "2.0184004306793213\n",
      "1.9622799158096313\n",
      "1.787206768989563\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = config.learning_rate, weight_decay = config.weight_decay, betas = (0.9, 0.99), eps = 1e-8)\n",
    "\n",
    "for i in range(3001):\n",
    "  bat = torch.randint(0, trnX.shape[0], (config.batch_size,))\n",
    "  _, loss = model(trnX[bat], trnY[bat])\n",
    "  model.zero_grad(set_to_none = True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if i % 1000 == 0: \n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "3 örnek eğitim setinde:\n",
      "--------------------------------------------------------------------------------\n",
      "karandere\n",
      "devret\n",
      "karacalar\n",
      "--------------------------------------------------------------------------------\n",
      "1 örnek validasyon setinde:\n",
      "--------------------------------------------------------------------------------\n",
      "celi\n",
      "--------------------------------------------------------------------------------\n",
      "46 örnek yeni:\n",
      "--------------------------------------------------------------------------------\n",
      "karaeş\n",
      "çiğşik\n",
      "behmancılık\n",
      "akşaytar\n",
      "karadım\n",
      "çatıalık\n",
      "kaynakçı\n",
      "elbey\n",
      "çivrekiş\n",
      "dançukuna\n",
      "çayhanlı\n",
      "şahrif\n",
      "kourunyurtlu\n",
      "taltar\n",
      "çanakdıncık\n",
      "yencigözü\n",
      "eliklevkuru\n",
      "kahkaten\n",
      "mizileliler\n",
      "mudgun\n",
      "etek\n",
      "kökser\n",
      "kalmak\n",
      "bozuni\n",
      "dakçulkırdın\n",
      "bekdivançır\n",
      "yivris\n",
      "yakaserli\n",
      "korumuşağı\n",
      "soğanbeyli\n",
      "muslur\n",
      "hacıan\n",
      "eldeytelik\n",
      "esamentış\n",
      "sevece\n",
      "camlan\n",
      "ilcacion\n",
      "düngimzir\n",
      "emiktepe\n",
      "eteköy\n",
      "karabek\n",
      "yaylıcak\n",
      "eskiötek\n",
      "yukarıalıoğlu\n",
      "balüydüec\n",
      "çörpekuyu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "inTrn, inTum, yeni = [], [], []\n",
    "for x in model.generate(50):\n",
    "  ornek = ''.join(idx2harf[h] for h in x)\n",
    "  if ornek in karisik:\n",
    "    inTrn.append(ornek)\n",
    "  elif ornek in isimler:\n",
    "    inTum.append(ornek)\n",
    "  else:\n",
    "    yeni.append(ornek)\n",
    "for lst, yer in [(inTrn, 'eğitim setinde'), (inTum, 'validasyon setinde'), (yeni, 'yeni')]:\n",
    "  if len(lst) == 0: continue\n",
    "  print('-'*80)\n",
    "  print(f\"{len(lst)} örnek {yer}:\")\n",
    "  print('-'*80)\n",
    "  for ornek in lst:\n",
    "    print(ornek)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
