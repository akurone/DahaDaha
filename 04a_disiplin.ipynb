{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daha Karmaşık Yapıların Temel Bileşeni: Disiplin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaldığımız Yer\n",
    "\n",
    "Önceki safhada biraz yol katettik fakat bu teknikle devam edersek belki çöp yerine isim üretebilen bir \"çöp\" üreteceğiz. En nihayetinde yapmaya çalıştığımız şey fonksiyon üreten bir fonksiyon; bunu inşa etmek için yaptığımız işlemleri soyutlayıp bir disiplin çerçevesinde düzenleyemezsek sisteme dahil edeceğimiz karmaşıklığı yönetemeyeceğiz. Bunun için neler yaptık bir bakalım:\n",
    "\n",
    "- Lineer denklemleri \"nöron\" olarak kullandık.\n",
    "- Aktivasyon için hiperbolik tanjant kullandık.\n",
    "- Harfleri 2 boyutta ifade etmek için `embedding` kullandık.\n",
    "- Bunları sıralı olarak işleme aldık.\n",
    "- Bu karmaşıklığı yönetemediğimiz için \"yığın normalleştirme\" işine bakmadık.\n",
    "\n",
    "Bu işlemleri ilk biz yapmadık ve bu karmaşıklık problemi ile ilk kez biz karşılaşmıyoruz; dolayısıyla bu yoldan bizden önce geçen fanilerin çıktılarından faydalanabiliriz. Mesela, [`torch.nn`](https://pytorch.org/docs/stable/nn.html) kütüphanesindeki bazı nesneleri (ki bu nesnelerde bu alanda yapılmış araştırmalardaki yöntemlerin soyut halleri) elle oluşturup onları kullanarak gerçekten çok katmanlı bir algılayıcı yapabiliriz.\n",
    "\n",
    "Ama önce yine bir [makale](https://arxiv.org/abs/1609.03499)! Konu farklı olsa da katmanlar arası iletişimin daha sağlıklı olması adına buradaki hiyerarşi kavramını kullanacağız.\n",
    "\n",
    "Aman dikkat! Ne kadar nöron, katman, hiyerarşi gibi soyutlamaları kullanıyor olsak da günün sonunda elimizde (bizim mütevazi boyutlarımızda bile 70 bin civarı parametresi olan) devasa bir fonksiyon var. Bu parametreleri \"eğitmek\" istiyoruz: elimizde çok daha devasa X-Y çiftleri var; parametreleri öyle düzenlemeliyiz ki `f(x) = y + z` olsun ve z olabildiğince küçük olsun. Ama aynı zamanda fonksiyon \"öğrensin\", \"ezberlemesin\" istiyoruz: eğitim setinde olmayan bir x için makul bir y üretebilsin. Bunu \"bütün\" haliyle yönetemediğimiz için kavramlara bölerek yönetmeye çalışıyoruz. Bu parçaları doğrudan birbirine bağladığımızda \"katman\" dediğimiz kavram anlamını yitiriyor. Bunun önüne geçmek için `tanh` gibi doğrusal akışı bozan ajanlar ekliyoruz. Bu gibi çözümler eğitim esnasında bazı nöronların \"kör\" olmasına neden oluyor. Bunu çözmek için (tek seferde tümünü kullanamayacağımız kadar çok olan) eğitim setinden oluşturduğumuz küçük yığınları normalize etmeye çalışıyoruz. Katman kavramı tekrar anlamlı oluyor fakat bu sefer katmanlar arası çok hızlı bilgi geçirdiğimiz için eğitim \"yavan\" oluyor. İşte bunu çözmek üzere arkadaşlar \"nöronlar\" önce bir kendi aralarında grup çalışmasıyla eğitimi pekiştirsinler, sonra \"üst katmana\" sonuçları arz etsinler gibi bir öneri getirmiş. Biz bunu `torch.nn` kütüphanesindeki `flatten` gibi uygulayacağız; arada ufak bir nüans olduğu için bizimki `FlattenConsecutive` olacak. Bahsettiğimiz normalizasyon işini ise `BatchNorm1d` yapacak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hazırlık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"hiper\" parametreler:\n",
    "n_blok = 8\n",
    "n_katman = 128\n",
    "n_embed = 16\n",
    "n_girdi = n_blok * n_embed\n",
    "n_batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29996, ['a', 'b', 'a', 'c', 'a', '.'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gerekli kütüphaneler\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# bunlar bize hep lazım\n",
    "alfabe = list('.abcçdefgğhıijklmnoöprsştuüvyz')\n",
    "harf2idx = { harf:idx for idx, harf in enumerate(alfabe) }\n",
    "idx2harf = { idx:harf for harf, idx in harf2idx.items() }\n",
    "\n",
    "# artık bigram ile işimiz yok. bize x y lazım ve buradaki x blok uzunluğunda harf dizisi, y ise bir sonraki harf\n",
    "def xyOlustur(isimler):\n",
    "  X, Y = [], []\n",
    "  for isim in isimler:\n",
    "    baglam = [0] * n_blok\n",
    "    for idx in map(lambda harf: harf2idx[harf], isim):\n",
    "      X.append(baglam)\n",
    "      Y.append(idx)\n",
    "      baglam = baglam[1:] + [idx]\n",
    "  X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "# isimleri okurken işleyelim\n",
    "isimler = list(map(lambda isim: list(isim) + ['.'], open(\"./isimler.txt\", \"r\").read().splitlines()))\n",
    "\n",
    "# adetlere devam:\n",
    "n_alfabe = len(alfabe)\n",
    "n_isim = len(isimler)\n",
    "n_isim, isimler[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri Kümeleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([282166, 8]) torch.Size([282166])\n",
      "torch.Size([225789, 8]) torch.Size([225789])\n"
     ]
    }
   ],
   "source": [
    "tumX, tumY = xyOlustur(isimler)\n",
    "random.seed(5)\n",
    "trnX, trnY = xyOlustur(random.sample(isimler, k = int(0.8 * n_isim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soyutlamalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "class Sequential:\n",
    "\n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "  def changeMode(self, training):\n",
    "    for layer in self.layers:\n",
    "      layer.training = training\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def getReady(self, weight = None):\n",
    "    n_param = 0\n",
    "    for l in self.layers:\n",
    "      l.training = True\n",
    "      for p in l.parameters():\n",
    "        p.requires_grad = True\n",
    "        n_param += p.nelement()\n",
    "      if weight is not None and hasattr(l, 'weight'):\n",
    "        l.weight *= weight\n",
    "    return n_param\n",
    "\n",
    "  def train(self, X, Y, lr):\n",
    "    backToEval = not self.layers[0].training\n",
    "    self.changeMode(True)\n",
    "    loss = F.cross_entropy(self(X), Y)\n",
    "    loss.backward()\n",
    "    for p in self.parameters():\n",
    "      p.data += -lr * p.grad\n",
    "      p.grad = None\n",
    "    if backToEval:\n",
    "      self.changeMode(False)\n",
    "    return loss\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def evalLoss(self, X, Y):\n",
    "    backToTraining = self.layers[0].training\n",
    "    self.changeMode(False)\n",
    "    loss = F.cross_entropy(self(X), Y)\n",
    "    if backToTraining:\n",
    "      self.changeMode(True)\n",
    "    return loss\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Linear:\n",
    "\n",
    "  def __init__(self, fan_in, fan_out, bias = False):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Embedding:\n",
    "\n",
    "  def __init__(self, num_embeddings, embedding_dim):\n",
    "    self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "\n",
    "  def __call__(self, IX):\n",
    "    self.out = self.weight[IX]\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.weight]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "\n",
    "  def __init__(self, dim, eps = 1e-5, momentum = 0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      if x.ndim == 2:\n",
    "        dim = 0\n",
    "      elif x.ndim == 3:\n",
    "        dim = (0,1)\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class FlattenConsecutive:\n",
    "\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "\n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape # batch, time, channels\n",
    "    x = x.view(B, T // self.n, C * self.n)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ve Parametreler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.4011964797973633"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(n_alfabe, n_embed),\n",
    "  FlattenConsecutive(2), Linear(n_embed  * 2, n_katman), BatchNorm1d(n_katman), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_katman * 2, n_katman), BatchNorm1d(n_katman), Tanh(),\n",
    "  FlattenConsecutive(2), Linear(n_katman * 2, n_katman), BatchNorm1d(n_katman), Tanh(),\n",
    "  Linear(n_katman, n_alfabe),\n",
    "])\n",
    "\n",
    "print(model.getReady(0.1)) # kaiming init: ağırlıkları azaltılarak başlatalım ki loss uçuk bir yerden başlamasın\n",
    "model.evalLoss(tumX, tumY).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eğitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn 3.4081709384918213 tum 3.4011030197143555\n",
      "trn 2.362731456756592 tum 2.3986237049102783\n",
      "trn 2.6051926612854004 tum 2.3217108249664307\n",
      "trn 2.3475451469421387 tum 2.23840069770813\n",
      "trn 2.3913252353668213 tum 2.1754918098449707\n",
      "trn 2.2034337520599365 tum 2.1586201190948486\n",
      "trn 2.04197096824646 tum 2.083258628845215\n",
      "trn 2.235935688018799 tum 2.071554660797119\n",
      "trn 1.894763708114624 tum 1.983964443206787\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "\n",
    "for i in range(9000):\n",
    "\n",
    "  bat = torch.randint(0, trnX.shape[0], (n_batch,))\n",
    "  batX, batY = trnX[bat], trnY[bat]\n",
    "\n",
    "  lr = 0.1 if i < 7500 else 0.01\n",
    "  loss = model.train(batX, batY, lr)\n",
    "\n",
    "  if i % 1000 == 0: \n",
    "    print('trn', loss.item(), 'tum', model.evalLoss(tumX, tumY).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sonuç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".karaç.\n",
      ".çomurlu.\n",
      ".karefentli.\n",
      ".megoğlu.\n",
      ".teseemin.\n",
      ".kızkılı.\n",
      ".hacıançay.\n",
      ".gücekçazıkçık.\n",
      ".beymercek.\n",
      ".umekli.\n",
      ".ıcıçıcat.\n",
      ".binat.\n",
      ".cemiti.\n",
      ".koyapınar.\n",
      ".bolanpınar.\n",
      ".akçılugöme.\n",
      ".kopralan.\n",
      ".hatunt.\n",
      ".asin.\n",
      ".bukçatoğun.\n",
      ".hacıamanlı.\n",
      ".setiran.\n",
      ".murdere.\n",
      ".gılıhpınar.\n",
      ".sesşinköy.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "model.changeMode(training = False)\n",
    "for _ in range(25):\n",
    "  ornek, baglam = [0], [0] * n_blok\n",
    "  while True:\n",
    "    P = F.softmax(model(torch.tensor([baglam])), dim = 1)\n",
    "    idx = torch.multinomial(P, num_samples = 1).item()\n",
    "    baglam = baglam[1:] + [idx]\n",
    "    ornek.append(idx)\n",
    "    if idx == 0:\n",
    "      break\n",
    "  print(''.join(idx2harf[i] for i in ornek))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
